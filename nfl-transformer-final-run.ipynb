{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kaggle.competitions import nflrush\n",
    "from tqdm import tqdm_notebook\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU, SELU, LeakyReLU\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook,tqdm,trange\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "import statsmodels.api as sm\n",
    "from scipy.interpolate import interp1d\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from scipy.stats import trim_mean\n",
    "import scipy\n",
    "from copy import deepcopy\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import constant_\n",
    "from torch.nn.init import xavier_normal_\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "import time\n",
    "start_time = time.time()\n",
    "time_limit = 9000\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "env = nflrush.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(721)\n",
    "os.environ['PYTHONHASHSEED'] = str(831)\n",
    "np.random.seed(101)\n",
    "torch.manual_seed(1111)\n",
    "torch.cuda.manual_seed(1117)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda:0\":\n",
    "    torch.cuda.set_device(0)\n",
    "    debug = True\n",
    "else:\n",
    "    debug = False\n",
    "debug = False\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #inlining this saves 1 second per epoch (V100 GPU) vs having a temp x and then returning x(!)\n",
    "        return x *( torch.tanh(F.softplus(x)))\n",
    "    \n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return gelu\n",
    "    else:\n",
    "        raise RuntimeError(\"activation should be relu/gelu, not %s.\" % activation)\n",
    "\n",
    "def multi_head_attention_forward(query,                           # type: Tensor\n",
    "                                 key,                             # type: Tensor\n",
    "                                 value,                           # type: Tensor\n",
    "                                 embed_dim_to_check,              # type: int\n",
    "                                 num_heads,                       # type: int\n",
    "                                 in_proj_weight,                  # type: Tensor\n",
    "                                 in_proj_bias,                    # type: Tensor\n",
    "                                 bias_k,                          # type: Optional[Tensor]\n",
    "                                 bias_v,                          # type: Optional[Tensor]\n",
    "                                 add_zero_attn,                   # type: bool\n",
    "                                 dropout_p,                       # type: float\n",
    "                                 out_proj_weight,                 # type: Tensor\n",
    "                                 out_proj_bias,                   # type: Tensor\n",
    "                                 training=True,                   # type: bool\n",
    "                                 key_padding_mask=None,           # type: Optional[Tensor]\n",
    "                                 need_weights=True,               # type: bool\n",
    "                                 attn_mask=None,                  # type: Optional[Tensor]\n",
    "                                 use_separate_proj_weight=False,  # type: bool\n",
    "                                 q_proj_weight=None,              # type: Optional[Tensor]\n",
    "                                 k_proj_weight=None,              # type: Optional[Tensor]\n",
    "                                 v_proj_weight=None,              # type: Optional[Tensor]\n",
    "                                 static_k=None,                   # type: Optional[Tensor]\n",
    "                                 static_v=None                    # type: Optional[Tensor]\n",
    "                                 ):\n",
    "    # type: (...) -> Tuple[Tensor, Optional[Tensor]]\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        embed_dim_to_check: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        in_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        dropout_p: probability of an element to be zeroed.\n",
    "        out_proj_weight, out_proj_bias: the output projection weight and bias.\n",
    "        training: apply dropout if is ``True``.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. This is an binary mask. When the value is True,\n",
    "            the corresponding value on the attention layer will be filled with -inf.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: mask that prevents attention to certain positions. This is an additive mask\n",
    "            (i.e. the values will be added to the attention layer).\n",
    "        use_separate_proj_weight: the function accept the proj. weights for query, key,\n",
    "            and value in differnt forms. If false, in_proj_weight will be used, which is\n",
    "            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n",
    "        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        static_k, static_v: static key and value used for attention operators.\n",
    "\n",
    "\n",
    "    Shape:\n",
    "        Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)`, ByteTensor, where N is the batch size, S is the source sequence length.\n",
    "        - attn_mask: :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "\n",
    "        Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "    \"\"\"\n",
    "\n",
    "    qkv_same = torch.equal(query, key) and torch.equal(key, value)\n",
    "    kv_same = torch.equal(key, value)\n",
    "\n",
    "    tgt_len, bsz, embed_dim = query.size()\n",
    "    assert embed_dim == embed_dim_to_check\n",
    "    assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
    "    assert key.size() == value.size()\n",
    "\n",
    "    head_dim = embed_dim // num_heads\n",
    "    assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "    scaling = float(head_dim) ** -0.5\n",
    "\n",
    "    if use_separate_proj_weight is not True:\n",
    "        if qkv_same:\n",
    "            # self-attention\n",
    "            q, k, v = F.linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)\n",
    "\n",
    "        elif kv_same:\n",
    "            # encoder-decoder attention\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = F.linear(query, _w, _b)\n",
    "\n",
    "            if key is None:\n",
    "                assert value is None\n",
    "                k = None\n",
    "                v = None\n",
    "            else:\n",
    "\n",
    "                # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "                _b = in_proj_bias\n",
    "                _start = embed_dim\n",
    "                _end = None\n",
    "                _w = in_proj_weight[_start:, :]\n",
    "                if _b is not None:\n",
    "                    _b = _b[_start:]\n",
    "                k, v = F.linear(key, _w, _b).chunk(2, dim=-1)\n",
    "\n",
    "        else:\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = F.linear(query, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim\n",
    "            _end = embed_dim * 2\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            k = F.linear(key, _w, _b)\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim * 2\n",
    "            _end = None\n",
    "            _w = in_proj_weight[_start:, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:]\n",
    "            v = F.linear(value, _w, _b)\n",
    "    else:\n",
    "        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)\n",
    "        len1, len2 = q_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == query.size(-1)\n",
    "\n",
    "        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)\n",
    "        len1, len2 = k_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == key.size(-1)\n",
    "\n",
    "        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)\n",
    "        len1, len2 = v_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == value.size(-1)\n",
    "\n",
    "        if in_proj_bias is not None:\n",
    "            q = F.linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])\n",
    "            k = F.linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim:(embed_dim * 2)])\n",
    "            v = F.linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2):])\n",
    "        else:\n",
    "            q = F.linear(query, q_proj_weight_non_opt, in_proj_bias)\n",
    "            k = F.linear(key, k_proj_weight_non_opt, in_proj_bias)\n",
    "            v = F.linear(value, v_proj_weight_non_opt, in_proj_bias)\n",
    "    q = q * scaling\n",
    "\n",
    "    if bias_k is not None and bias_v is not None:\n",
    "        if static_k is None and static_v is None:\n",
    "            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = torch.cat([attn_mask,\n",
    "                                      torch.zeros((attn_mask.size(0), 1),\n",
    "                                                  dtype=attn_mask.dtype,\n",
    "                                                  device=attn_mask.device)], dim=1)\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = torch.cat(\n",
    "                    [key_padding_mask, torch.zeros((key_padding_mask.size(0), 1),\n",
    "                                                   dtype=key_padding_mask.dtype,\n",
    "                                                   device=key_padding_mask.device)], dim=1)\n",
    "        else:\n",
    "            assert static_k is None, \"bias cannot be added to static key.\"\n",
    "            assert static_v is None, \"bias cannot be added to static value.\"\n",
    "    else:\n",
    "        assert bias_k is None\n",
    "        assert bias_v is None\n",
    "\n",
    "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if k is not None:\n",
    "        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if v is not None:\n",
    "        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    if static_k is not None:\n",
    "        assert static_k.size(0) == bsz * num_heads\n",
    "        assert static_k.size(2) == head_dim\n",
    "        k = static_k\n",
    "\n",
    "    if static_v is not None:\n",
    "        assert static_v.size(0) == bsz * num_heads\n",
    "        assert static_v.size(2) == head_dim\n",
    "        v = static_v\n",
    "\n",
    "    src_len = k.size(1)\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.size(0) == bsz\n",
    "        assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "    if add_zero_attn:\n",
    "        src_len += 1\n",
    "        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)\n",
    "        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = torch.cat([attn_mask, torch.zeros((attn_mask.size(0), 1),\n",
    "                                                          dtype=attn_mask.dtype,\n",
    "                                                          device=attn_mask.device)], dim=1)\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = torch.cat(\n",
    "                [key_padding_mask, torch.zeros((key_padding_mask.size(0), 1),\n",
    "                                               dtype=key_padding_mask.dtype,\n",
    "                                               device=key_padding_mask.device)], dim=1)\n",
    "\n",
    "    attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        attn_mask = attn_mask.unsqueeze(0)\n",
    "        attn_output_weights += attn_mask\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        attn_output_weights = attn_output_weights.masked_fill(\n",
    "            key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "            float('-inf'),\n",
    "        )\n",
    "        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)\n",
    "\n",
    "    attn_output_weights = F.softmax(\n",
    "        attn_output_weights, dim=-1)\n",
    "    attn_output_weights = F.dropout(attn_output_weights, p=dropout_p, training=training)\n",
    "\n",
    "    attn_output = torch.bmm(attn_output_weights, v)\n",
    "    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "    attn_output = F.linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "\n",
    "    if need_weights:\n",
    "        # average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
    "    else:\n",
    "        return attn_output, None\n",
    "\n",
    "        \n",
    "class MultiheadAttention(nn.Module):\n",
    "    r\"\"\"Allows the model to jointly attend to information\n",
    "    from different representation subspaces.\n",
    "    See reference: Attention Is All You Need\n",
    "\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\n",
    "    Args:\n",
    "        embed_dim: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "        bias: add bias as module parameter. Default: True.\n",
    "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        kdim: total number of features in key. Default: None.\n",
    "        vdim: total number of features in key. Default: None.\n",
    "\n",
    "        Note: if kdim and vdim are None, they will be set to embed_dim such that\n",
    "        query, key, and value have the same number of features.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.in_proj_weight = Parameter(torch.empty(3 * embed_dim, embed_dim))\n",
    "\n",
    "        if self._qkv_same_embed_dim is False:\n",
    "            self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))\n",
    "            self.k_proj_weight = Parameter(torch.Tensor(embed_dim, self.kdim))\n",
    "            self.v_proj_weight = Parameter(torch.Tensor(embed_dim, self.vdim))\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = Parameter(torch.empty(1, 1, embed_dim))\n",
    "            self.bias_v = Parameter(torch.empty(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self._qkv_same_embed_dim:\n",
    "            xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            xavier_uniform_(self.q_proj_weight)\n",
    "            xavier_uniform_(self.k_proj_weight)\n",
    "            xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            constant_(self.in_proj_bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            xavier_normal_(self.bias_v)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None,\n",
    "                need_weights=True, attn_mask=None):\n",
    "        r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. This is an binary mask. When the value is True,\n",
    "            the corresponding value on the attention layer will be filled with -inf.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: mask that prevents attention to certain positions. This is an additive mask\n",
    "            (i.e. the values will be added to the attention layer).  \n",
    "\n",
    "    Shape:\n",
    "        - Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)`, ByteTensor, where N is the batch size, S is the source sequence length.\n",
    "        - attn_mask: :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "        \"\"\"\n",
    "        if hasattr(self, '_qkv_same_embed_dim') and self._qkv_same_embed_dim is False:\n",
    "            return multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias, \n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights, \n",
    "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                v_proj_weight=self.v_proj_weight)\n",
    "        else:\n",
    "            if not hasattr(self, '_qkv_same_embed_dim'):\n",
    "                warnings.warn('A new version of MultiheadAttention module has been implemented. \\\n",
    "                    Please re-train your model with the new module',\n",
    "                              UserWarning)\n",
    "\n",
    "            return multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias, \n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights, \n",
    "                attn_mask=attn_mask)\n",
    "        \n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "#         self.activation = Mish()\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        r\"\"\"Pass the input through the endocder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequnce to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        if hasattr(self, \"activation\"):\n",
    "            src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        else:  # for backward compatibility\n",
    "            src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "    \n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    r\"\"\"TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n",
    "    This standard decoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "\n",
    "    Examples::\n",
    "        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        >>> memory = torch.rand(10, 32, 512)\n",
    "        >>> tgt = torch.rand(20, 32, 512)\n",
    "        >>> out = decoder_layer(tgt, memory)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        r\"\"\"Pass the inputs (and mask) through the decoder layer.\n",
    "\n",
    "        Args:\n",
    "            tgt: the sequence to the decoder layer (required).\n",
    "            memory: the sequnce from the last layer of the encoder (required).\n",
    "            tgt_mask: the mask for the tgt sequence (optional).\n",
    "            memory_mask: the mask for the memory sequence (optional).\n",
    "            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        if hasattr(self, \"activation\"):\n",
    "            tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        else:  # for backward compatibility\n",
    "            tgt2 = self.linear2(self.dropout(F.relu(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "    \n",
    "    \n",
    "class RAdam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:            \n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "    \n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "    \n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['IsRusher',\n",
    "            'IsRusherTeam',\n",
    "            'X','Y',\n",
    "            'Dir_X','Dir_Y',\n",
    "            'Orientation_X','Orientation_Y',\n",
    "            'S',\n",
    "            'DistanceToBall',\n",
    "            'BallDistanceX','BallDistanceY',\n",
    "            'BallAngleX','BallAngleY',\n",
    "            'related_horizontal_v','related_vertical_v',\n",
    "            'related_horizontal_A','related_vertical_A',\n",
    "#             'horizontal_v','vertical_v',\n",
    "#             'horizontal_A','vertical_A',\n",
    "            'TeamDistance','EnermyTeamDistance',\n",
    "            'TeamXstd','EnermyXstd',\n",
    "            'EnermyYstd','TeamYstd',\n",
    "            'DistanceToBallRank',\n",
    "            'DistanceToBallRank_AttTeam','DistanceToBallRank_DefTeam',\n",
    "            'YardLine',\n",
    "            'NextX','NextY',\n",
    "            'NextDistanceToBall',\n",
    "            'BallNextAngleX','BallNextAngleY',\n",
    "            'BallNextDistanceX','BallNextDistanceY',\n",
    "            'A',\n",
    "#             'Dis0',\n",
    "#             'Season',\n",
    "#             'problem_pos',\n",
    "#             'PlayDirection',\n",
    "#             '1YardToYardLine',\n",
    "#             'AttCloseYardLine','DefCloseYardLine'\n",
    "           ]\n",
    "\n",
    "map_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\n",
    "for abb in train_df['PossessionTeam'].unique():\n",
    "    map_abbr[abb] = abb\n",
    "\n",
    "# problem_pos = ['SS','FS','CB','WR']\n",
    "    \n",
    "def Create_group_features(df,train=True):\n",
    "    df['PossessionTeam'] = df['PossessionTeam'].map(map_abbr)\n",
    "    df['HomeTeamAbbr'] = df['HomeTeamAbbr'].map(map_abbr)\n",
    "    df['Season'] = (df['Season'] == 2017)\n",
    "    df.loc[df['Season'],'A'] = np.nan\n",
    "    df['S'] = 10*df['Dis']\n",
    "#     df.loc[df['Season']&(df['Dis']==0),'S']=np.nan\n",
    "#     df['Dis0'] = df['Dis'] == 0\n",
    "#     df.loc[df['Dis0'],'S']=np.nan\n",
    "#     df.loc[df['Dis0'],'A']=np.nan\n",
    "    df.loc[df['Season'],'Orientation'] = np.mod(90+df.loc[df['Season'],'Orientation'],360)\n",
    "    mask = df['PlayDirection'] == 'left'\n",
    "    df.loc[mask, 'X'] = 120 - df.loc[mask, 'X']\n",
    "    df.loc[mask, 'Y'] = (160 / 3) - df.loc[mask, 'Y']\n",
    "    df.loc[mask, 'Orientation'] = np.mod(180 + df.loc[mask, 'Orientation'], 360)\n",
    "    df.loc[mask, 'Dir'] = np.mod(180 + df.loc[mask, 'Dir'], 360)\n",
    "    df['FieldPosition'].fillna('', inplace=True)\n",
    "    mask = df['PossessionTeam'] != df['FieldPosition']\n",
    "    df.loc[mask, 'YardLine'] = 100 - df.loc[mask, 'YardLine']\n",
    "#     df['1YardToYardLine'] = (df['X'] - (10+df['YardLine'])).abs() < 1\n",
    "    df['PlayDirection'] = df['PlayDirection'].map({'left':1,'right':0})\n",
    "    df['Team'] = df['Team'].map({'away':1,'home':0})\n",
    "    df['Dir'] = np.mod(90+df['Dir'],360)\n",
    "    df['Orientation'] = np.mod(90+df['Orientation'],360)\n",
    "    df['Orientation_X'] = np.cos(df['Orientation']*2*np.pi/360)\n",
    "    df['Orientation_Y'] = np.sin(df['Orientation']*2*np.pi/360)\n",
    "    df['Dir_X'] = np.cos(df['Dir']*2*np.pi/360)\n",
    "    df['Dir_Y'] = np.sin(df['Dir']*2*np.pi/360)\n",
    "    df['IsRusher'] = (df['NflId'] == df['NflIdRusher'])\n",
    "    rushers = df[df['IsRusher']][['PlayId', 'Team', 'X', 'Y', 'A', 'S', 'Dir_X','Dir_Y']]\n",
    "    rushers.columns = ['PlayId', 'RusherTeam', 'BallX', 'BallY', 'BallA', 'BallS', 'BallDir_X', 'BallDir_Y']\n",
    "    df = df.join(rushers.set_index('PlayId'), on='PlayId')\n",
    "    df['IsRusherTeam'] = (df['Team'] == df['RusherTeam'])\n",
    "    df['DistanceToBall'] = ((df['X'] - df['BallX'])**2 + (df['Y'] - df['BallY'])**2 )**0.5\n",
    "    dx = df['BallX']-df['X']\n",
    "    dy = df['BallY']-df['Y']\n",
    "    angle = np.arctan(dy/(1e-8+dx))*360/(2*np.pi)\n",
    "    angle[dx<0] += 180\n",
    "    angle[(dx>0)&(dy<0)] += 360\n",
    "    angle = np.mod(angle,360)\n",
    "    df['BallAngleX'] = np.sin(angle*2*np.pi/360)\n",
    "    df['BallAngleY'] = np.cos(angle*2*np.pi/360)\n",
    "    df['BallDistanceX'] = -dx\n",
    "    df['BallDistanceY'] = -dy\n",
    "    df['related_horizontal_v'] = df['S']*df['Dir_X'] - df['BallS']*df['BallDir_X']\n",
    "    df['related_vertical_v'] = df['S']*df['Dir_Y'] - df['BallS']*df['BallDir_Y']\n",
    "    df['related_horizontal_A'] = df['A']*df['Dir_X'] - df['BallA']*df['BallDir_X']\n",
    "    df['related_vertical_A'] = df['A']*df['Dir_Y'] - df['BallA']*df['BallDir_Y']\n",
    "    df['DistanceToBallRank'] = df.groupby('PlayId')['DistanceToBall'].rank()\n",
    "    Distance_Ball_Team = df.groupby(['PlayId','Team'])['DistanceToBall'].rank()\n",
    "    df['DistanceToBallRank_AttTeam'] = np.nan\n",
    "    df['DistanceToBallRank_DefTeam'] = np.nan\n",
    "    df.loc[df['IsRusherTeam'],'DistanceToBallRank_AttTeam'] = Distance_Ball_Team.loc[df['IsRusherTeam']]\n",
    "    df.loc[~df['IsRusherTeam'],'DistanceToBallRank_DefTeam'] = Distance_Ball_Team.loc[~df['IsRusherTeam']]\n",
    "    TeamXY = df.groupby(['PlayId','Team'])[['X','Y']].transform(np.mean)\n",
    "    df['TeamDistance'] = ((df['X'] - TeamXY['X'])**2 + (df['Y'] - TeamXY['Y'])**2 )**0.5\n",
    "    df['TeamXstd'] = df['X'] - TeamXY['X']\n",
    "    df['TeamYstd'] = df['Y'] - TeamXY['Y']\n",
    "    EnermyTeamXY = TeamXY.groupby(df['PlayId']).apply(lambda x: x.iloc[::-1]).values\n",
    "    EnermyTeamXY = pd.DataFrame(EnermyTeamXY,index = TeamXY.index,columns = TeamXY.columns)\n",
    "    df['EnermyTeamDistance'] = ((df['X'] - EnermyTeamXY['X'])**2 + (df['Y'] - EnermyTeamXY['Y'])**2 )**0.5\n",
    "    df['EnermyXstd'] = df['X'] - EnermyTeamXY['X']\n",
    "    df['EnermyYstd'] = df['Y'] - EnermyTeamXY['Y']\n",
    "    df['NextX'] = df['S']*df['Dir_X']+df['X']\n",
    "    df['NextY'] = df['S']*df['Dir_Y']+df['Y']\n",
    "    rushers = df[df['IsRusher']][['PlayId', 'NextX', 'NextY']]\n",
    "    rushers.columns = ['PlayId', 'BallNextX', 'BallNextY']\n",
    "    df = df.join(rushers.set_index('PlayId'), on='PlayId')\n",
    "    df['NextDistanceToBall'] = ((df['NextX'] - df['BallNextX'])**2 + (df['NextY'] - df['BallNextY'])**2 )**0.5\n",
    "    dx = df['BallNextX']-df['NextX']\n",
    "    dy = df['BallNextY']-df['NextY']\n",
    "    angle = np.arctan(dy/(1e-8+dx))*360/(2*np.pi)\n",
    "    angle[dx<0] += 180\n",
    "    angle[(dx>0)&(dy<0)] += 360\n",
    "    angle = np.mod(angle,360)\n",
    "    df['BallNextAngleX'] = np.sin(angle*2*np.pi/360)\n",
    "    df['BallNextAngleY'] = np.cos(angle*2*np.pi/360)\n",
    "    df['BallNextDistanceX'] = -dx\n",
    "    df['BallNextDistanceY'] = -dy\n",
    "#     df['problem_pos'] = df['Position'].isin(problem_pos)\n",
    "#     YL_Dis = df.groupby(['PlayId','IsRusherTeam'])['1YardToYardLine'].transform(np.sum)\n",
    "#     df['AttCloseYardLine'] = np.nan\n",
    "#     df['DefCloseYardLine'] = np.nan\n",
    "#     df.loc[df['IsRusherTeam'],'AttCloseYardLine'] = YL_Dis.loc[df['IsRusherTeam']]\n",
    "#     df.loc[~df['IsRusherTeam'],'DefCloseYardLine'] = YL_Dis.loc[~df['IsRusherTeam']]\n",
    "    \n",
    "#     df['horizontal_v']=df['S']*df['Dir_X']\n",
    "#     df['vertical_v']=df['S']*df['Dir_Y']\n",
    "#     df['horizontal_A']=df['A']*df['Dir_X']\n",
    "#     df['vertical_A']=df['A']*df['Dir_Y']\n",
    "#     df['SA']=df['S']*df['A']\n",
    "    return df[num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.5 s, sys: 4.04 s, total: 30.5 s\n",
      "Wall time: 28 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23171, 22, 36)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "groups = (train_df['GameId'].astype(str).str[:4] + train_df['Week'].astype(str))\n",
    "groups = groups.groupby(train_df['PlayId']).apply(lambda x:x.iloc[0]).values\n",
    "train_y = train_df['Yards'].groupby(train_df['PlayId']).apply(lambda x:x.iloc[0]).values\n",
    "train_X_df = Create_group_features(train_df.copy())\n",
    "bool_cols = ['IsRusher','IsRusherTeam']\n",
    "Scale_cols = [col for col in train_X_df.columns if col not in bool_cols]\n",
    "X_mean = train_X_df[Scale_cols].mean()\n",
    "X_std = train_X_df[Scale_cols].std()\n",
    "train_X = train_X_df.copy()\n",
    "train_X[bool_cols] = train_X[bool_cols].astype(np.float32)\n",
    "train_X[Scale_cols] = (train_X_df[Scale_cols] - X_mean[Scale_cols])/X_std[Scale_cols]\n",
    "train_X.fillna(0,inplace=True)\n",
    "train_X = train_X.values.reshape(len(train_y),22,-1)\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IsRusher</th>\n",
       "      <th>IsRusherTeam</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Dir_X</th>\n",
       "      <th>Dir_Y</th>\n",
       "      <th>Orientation_X</th>\n",
       "      <th>Orientation_Y</th>\n",
       "      <th>S</th>\n",
       "      <th>DistanceToBall</th>\n",
       "      <th>...</th>\n",
       "      <th>DistanceToBallRank_DefTeam</th>\n",
       "      <th>YardLine</th>\n",
       "      <th>NextX</th>\n",
       "      <th>NextY</th>\n",
       "      <th>NextDistanceToBall</th>\n",
       "      <th>BallNextAngleX</th>\n",
       "      <th>BallNextAngleY</th>\n",
       "      <th>BallNextDistanceX</th>\n",
       "      <th>BallNextDistanceY</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>46.09</td>\n",
       "      <td>18.493333</td>\n",
       "      <td>0.049198</td>\n",
       "      <td>0.998789</td>\n",
       "      <td>0.139346</td>\n",
       "      <td>0.990244</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.480872</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>35</td>\n",
       "      <td>46.286794</td>\n",
       "      <td>22.488489</td>\n",
       "      <td>8.705788</td>\n",
       "      <td>0.215510</td>\n",
       "      <td>-0.976502</td>\n",
       "      <td>8.501217</td>\n",
       "      <td>-1.876180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>45.33</td>\n",
       "      <td>20.693333</td>\n",
       "      <td>-0.320613</td>\n",
       "      <td>0.947210</td>\n",
       "      <td>0.886123</td>\n",
       "      <td>0.463451</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.593310</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35</td>\n",
       "      <td>45.297939</td>\n",
       "      <td>20.788054</td>\n",
       "      <td>8.320322</td>\n",
       "      <td>0.429865</td>\n",
       "      <td>-0.902893</td>\n",
       "      <td>7.512362</td>\n",
       "      <td>-3.576615</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>46.00</td>\n",
       "      <td>20.133333</td>\n",
       "      <td>-0.386389</td>\n",
       "      <td>0.922336</td>\n",
       "      <td>0.998620</td>\n",
       "      <td>0.052510</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.448982</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35</td>\n",
       "      <td>44.802194</td>\n",
       "      <td>22.992575</td>\n",
       "      <td>7.149515</td>\n",
       "      <td>0.191914</td>\n",
       "      <td>-0.981412</td>\n",
       "      <td>7.016617</td>\n",
       "      <td>-1.372095</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>48.54</td>\n",
       "      <td>25.633333</td>\n",
       "      <td>0.962975</td>\n",
       "      <td>0.269592</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>-0.004014</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7.820038</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>35</td>\n",
       "      <td>48.732595</td>\n",
       "      <td>25.687252</td>\n",
       "      <td>11.026624</td>\n",
       "      <td>-0.119944</td>\n",
       "      <td>-0.992781</td>\n",
       "      <td>10.947018</td>\n",
       "      <td>1.322582</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>50.68</td>\n",
       "      <td>17.913333</td>\n",
       "      <td>0.270432</td>\n",
       "      <td>0.962739</td>\n",
       "      <td>0.975802</td>\n",
       "      <td>0.218654</td>\n",
       "      <td>1.6</td>\n",
       "      <td>10.622476</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>35</td>\n",
       "      <td>51.112692</td>\n",
       "      <td>19.453716</td>\n",
       "      <td>14.203150</td>\n",
       "      <td>0.345765</td>\n",
       "      <td>-0.938321</td>\n",
       "      <td>13.327115</td>\n",
       "      <td>-4.910954</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509757</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>86.77</td>\n",
       "      <td>24.200000</td>\n",
       "      <td>-0.264041</td>\n",
       "      <td>0.964511</td>\n",
       "      <td>-0.701531</td>\n",
       "      <td>0.712639</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.345345</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77</td>\n",
       "      <td>86.189109</td>\n",
       "      <td>26.321925</td>\n",
       "      <td>9.586588</td>\n",
       "      <td>-0.219178</td>\n",
       "      <td>-0.975685</td>\n",
       "      <td>9.353489</td>\n",
       "      <td>2.101172</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509758</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>86.76</td>\n",
       "      <td>27.180000</td>\n",
       "      <td>-0.679441</td>\n",
       "      <td>0.733730</td>\n",
       "      <td>-0.805204</td>\n",
       "      <td>0.592997</td>\n",
       "      <td>1.1</td>\n",
       "      <td>6.017516</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77</td>\n",
       "      <td>86.012615</td>\n",
       "      <td>27.987103</td>\n",
       "      <td>9.919809</td>\n",
       "      <td>-0.379680</td>\n",
       "      <td>-0.925118</td>\n",
       "      <td>9.176995</td>\n",
       "      <td>3.766349</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509759</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>87.26</td>\n",
       "      <td>27.050000</td>\n",
       "      <td>-0.360322</td>\n",
       "      <td>0.932828</td>\n",
       "      <td>-0.069060</td>\n",
       "      <td>0.997613</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.497815</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77</td>\n",
       "      <td>86.323162</td>\n",
       "      <td>29.475352</td>\n",
       "      <td>10.845472</td>\n",
       "      <td>-0.484497</td>\n",
       "      <td>-0.874793</td>\n",
       "      <td>9.487542</td>\n",
       "      <td>5.254599</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509760</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>84.57</td>\n",
       "      <td>24.370000</td>\n",
       "      <td>0.058261</td>\n",
       "      <td>-0.998301</td>\n",
       "      <td>-0.528735</td>\n",
       "      <td>-0.848787</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.258321</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77</td>\n",
       "      <td>84.843827</td>\n",
       "      <td>19.677983</td>\n",
       "      <td>9.206961</td>\n",
       "      <td>0.493406</td>\n",
       "      <td>-0.869799</td>\n",
       "      <td>8.008207</td>\n",
       "      <td>-4.542770</td>\n",
       "      <td>1.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509761</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>80.80</td>\n",
       "      <td>26.350000</td>\n",
       "      <td>-0.880973</td>\n",
       "      <td>-0.473166</td>\n",
       "      <td>-0.701656</td>\n",
       "      <td>-0.712516</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77</td>\n",
       "      <td>76.835620</td>\n",
       "      <td>24.220753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>4.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509762 rows  36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        IsRusher  IsRusherTeam      X          Y     Dir_X     Dir_Y  \\\n",
       "0          False         False  46.09  18.493333  0.049198  0.998789   \n",
       "1          False         False  45.33  20.693333 -0.320613  0.947210   \n",
       "2          False         False  46.00  20.133333 -0.386389  0.922336   \n",
       "3          False         False  48.54  25.633333  0.962975  0.269592   \n",
       "4          False         False  50.68  17.913333  0.270432  0.962739   \n",
       "...          ...           ...    ...        ...       ...       ...   \n",
       "509757     False          True  86.77  24.200000 -0.264041  0.964511   \n",
       "509758     False          True  86.76  27.180000 -0.679441  0.733730   \n",
       "509759     False          True  87.26  27.050000 -0.360322  0.932828   \n",
       "509760     False          True  84.57  24.370000  0.058261 -0.998301   \n",
       "509761      True          True  80.80  26.350000 -0.880973 -0.473166   \n",
       "\n",
       "        Orientation_X  Orientation_Y    S  DistanceToBall  ...  \\\n",
       "0            0.139346       0.990244  4.0        6.480872  ...   \n",
       "1            0.886123       0.463451  0.1        4.593310  ...   \n",
       "2            0.998620       0.052510  3.1        5.448982  ...   \n",
       "3            0.999992      -0.004014  0.2        7.820038  ...   \n",
       "4            0.975802       0.218654  1.6       10.622476  ...   \n",
       "...               ...            ...  ...             ...  ...   \n",
       "509757      -0.701531       0.712639  2.2        6.345345  ...   \n",
       "509758      -0.805204       0.592997  1.1        6.017516  ...   \n",
       "509759      -0.069060       0.997613  2.6        6.497815  ...   \n",
       "509760      -0.528735      -0.848787  4.7        4.258321  ...   \n",
       "509761      -0.701656      -0.712516  4.5        0.000000  ...   \n",
       "\n",
       "        DistanceToBallRank_DefTeam  YardLine      NextX      NextY  \\\n",
       "0                              4.0        35  46.286794  22.488489   \n",
       "1                              1.0        35  45.297939  20.788054   \n",
       "2                              3.0        35  44.802194  22.992575   \n",
       "3                              6.0        35  48.732595  25.687252   \n",
       "4                              8.0        35  51.112692  19.453716   \n",
       "...                            ...       ...        ...        ...   \n",
       "509757                         NaN        77  86.189109  26.321925   \n",
       "509758                         NaN        77  86.012615  27.987103   \n",
       "509759                         NaN        77  86.323162  29.475352   \n",
       "509760                         NaN        77  84.843827  19.677983   \n",
       "509761                         NaN        77  76.835620  24.220753   \n",
       "\n",
       "        NextDistanceToBall  BallNextAngleX  BallNextAngleY  BallNextDistanceX  \\\n",
       "0                 8.705788        0.215510       -0.976502           8.501217   \n",
       "1                 8.320322        0.429865       -0.902893           7.512362   \n",
       "2                 7.149515        0.191914       -0.981412           7.016617   \n",
       "3                11.026624       -0.119944       -0.992781          10.947018   \n",
       "4                14.203150        0.345765       -0.938321          13.327115   \n",
       "...                    ...             ...             ...                ...   \n",
       "509757            9.586588       -0.219178       -0.975685           9.353489   \n",
       "509758            9.919809       -0.379680       -0.925118           9.176995   \n",
       "509759           10.845472       -0.484497       -0.874793           9.487542   \n",
       "509760            9.206961        0.493406       -0.869799           8.008207   \n",
       "509761            0.000000        0.000000        1.000000          -0.000000   \n",
       "\n",
       "        BallNextDistanceY     A  \n",
       "0               -1.876180   NaN  \n",
       "1               -3.576615   NaN  \n",
       "2               -1.372095   NaN  \n",
       "3                1.322582   NaN  \n",
       "4               -4.910954   NaN  \n",
       "...                   ...   ...  \n",
       "509757           2.101172  2.12  \n",
       "509758           3.766349  0.66  \n",
       "509759           5.254599  1.18  \n",
       "509760          -4.542770  1.79  \n",
       "509761          -0.000000  4.10  \n",
       "\n",
       "[509762 rows x 36 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.arange(-99, 100)\n",
    "n = np.row_stack([n] * train_y.shape[0])\n",
    "ym = train_y.reshape(train_y.shape[0], 1)\n",
    "train_y = np.heaviside(n - ym, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lower_yards = np.heaviside(99 - train_X_df['YardLine'].values[::22].reshape(train_y.shape[0], 1) - np.row_stack([np.arange(199)] * train_y.shape[0]),0)\n",
    "Upper_yards = np.heaviside(np.row_stack([np.arange(199)] * train_y.shape[0]) - 199 + train_X_df['YardLine'].values[::22].reshape(train_y.shape[0], 1),0)\n",
    "Yard_mask = Lower_yards + Upper_yards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_OH = np.zeros([train_y.shape[0],199])\n",
    "train_y_OH[np.arange(len(ym)), (ym+99).reshape(-1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,params):\n",
    "        super(Model, self).__init__()\n",
    "        Activation = params['Activation']\n",
    "        self.layers = params['layer']\n",
    "        \n",
    "        self.emb = Seq(nn.Dropout(params['dropout_0']),Lin(params['input_size'],params['hidden_0']))\n",
    "#         self.emb = Seq(nn.Dropout(params['dropout_0']),Lin(params['input_size'],params['hidden_3']),Activation,Lin(params['hidden_3'],params['hidden_0']))\n",
    "        self.transformer_encoder = nn.ModuleList()\n",
    "        self.transformer_decoder = nn.ModuleList()\n",
    "        for l in range(self.layers):\n",
    "            self.transformer_encoder.append(TransformerEncoderLayer(params['hidden_0'],\\\n",
    "                                                       nhead=params['head'],\\\n",
    "                                                       dim_feedforward=params['hidden_1'],\\\n",
    "                                                       dropout=params['dropout_1'],\\\n",
    "                                                       activation = params['transformer_activation']))\n",
    "            self.transformer_decoder.append(TransformerDecoderLayer(params['hidden_0'],\\\n",
    "                                                   nhead=params['head'],\\\n",
    "                                                   dim_feedforward=params['hidden_1'],\\\n",
    "                                                   dropout=params['dropout_1'],\\\n",
    "                                                   activation = params['transformer_activation']))\n",
    "\n",
    "        self.mlp = Seq(Lin(params['hidden_0'], params['hidden_3']),Activation,nn.Dropout(params['dropout_3']),Lin(params['hidden_3'], 199))\n",
    "\n",
    "    def forward(self, X):\n",
    "        rusher_mask = X[:,:,0]>0\n",
    "        combine = self.emb(X)\n",
    "        x = combine.permute(1,0,2)\n",
    "        for l in range(self.layers):\n",
    "            x = self.transformer_encoder[l](x)\n",
    "        rusher = (x.permute(1,0,2)[rusher_mask].unsqueeze(1)).permute(1,0,2)\n",
    "        for l in range(self.layers):\n",
    "            rusher = self.transformer_decoder[l](rusher,x)\n",
    "        x = rusher.permute(1,0,2)\n",
    "        y = x.squeeze(1)\n",
    "        y = self.mlp(y)\n",
    "        y = F.softmax(y,dim=-1)\n",
    "        return y,torch.cumsum(y,dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Schedule_scheme(x):\n",
    "    if x%3==0:\n",
    "        return 1\n",
    "    elif x%3==1:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0.2\n",
    "\n",
    "def Create_model(params,input_size = train_X.shape[-1]):\n",
    "    params['input_size'] = input_size\n",
    "    clf = Model(params).to(device)\n",
    "    vv = 0\n",
    "    for v in list(clf.parameters()):\n",
    "        vv += len(v.view(-1))\n",
    "    print(\"Num_Variables:\",vv)\n",
    "    base_opt = RAdam(clf.parameters(),lr=params['lr'],\\\n",
    "                     weight_decay=params['weight_decay'],\\\n",
    "                     betas=(1-params['alpha'], 1-params['beta']), eps=params['eps']\n",
    "                    )\n",
    "    optimizer = Lookahead(base_opt, k=params['lookahead_k'], alpha=params['lookahead_alpha'])\n",
    "#     scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda= lambda x:  0.5*(1 + np.cos(np.pi*np.mod(x,params['cosine_T'])/params['cosine_T'])))\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda= lambda x: 1 if x%2==0 else 0.5)\n",
    "#     scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda= Schedule_scheme)\n",
    "    return clf,optimizer,scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'Activation': 'lrelu1',\n",
    "          'alpha': 0.06177912913861566,\n",
    "          'batch_size': 82,\n",
    "          'beta': 0.025662159487370977,\n",
    "          'cosine_T': 67,\n",
    "          'dropout_0': 0.023898897998374913,\n",
    "          'dropout_1': 0.06080646510619758,\n",
    "          'dropout_2': 0.025011299009768506,\n",
    "          'dropout_3': 0.22165717850445166,\n",
    "          'eps': 5.884062604808768e-07,\n",
    "          'head': 6,\n",
    "          'hidden_0': 111,\n",
    "          'hidden_1': 2040,\n",
    "          'hidden_2': 1742,\n",
    "          'hidden_3': 933,\n",
    "          'layer': 4,\n",
    "          'lookahead_alpha': 0.5422623511158232,\n",
    "          'lookahead_k': 3,\n",
    "          'loss_1': 0.48305609964511376,\n",
    "          'loss_2': 0.2353079388007755,\n",
    "          'lr': 0.00014789689708999082,\n",
    "          'min_lr': 0.0038268377963604707,\n",
    "          'transformer_activation': 'gelu',\n",
    "          'weight_decay': 2.2445535063229495e-05}\n",
    "\n",
    "act_map = {'lrelu1':LeakyReLU(0.1),\n",
    "           'lrelu2':LeakyReLU(0.2),\n",
    "           'relu':ReLU(),\n",
    "           'selu':SELU(),\n",
    "           'prelu':nn.PReLU(),\n",
    "           'elu':nn.ELU(),\n",
    "           'mish':Mish()}\n",
    "\n",
    "params = {'Activation': Mish(),\n",
    "              'dropout_0': config['dropout_0'],\n",
    "              'hidden_0': 128,\n",
    "              'head': 32,\n",
    "              'hidden_1': 512,\n",
    "              \"dropout_1\": 0.2,\n",
    "              'transformer_activation': config['transformer_activation'],\n",
    "              \"hidden_2\": config['hidden_2'],\n",
    "              \"dropout_2\": config['dropout_2'],\n",
    "              'hidden_3': 512,\n",
    "              'dropout_3': 0.3,\n",
    "              \"layer\": 2,\n",
    "              \"lr\": 5*config['lr'],\n",
    "              'weight_decay': 1e-1,\n",
    "              'alpha': config['alpha'],\n",
    "              'beta': config['beta'],\n",
    "              'eps': config['eps'],\n",
    "              'lookahead_k': 5,\n",
    "              'lookahead_alpha': 0.5,\n",
    "              'cosine_T': config['cosine_T'],\n",
    "              'min_lr': 0.01*config['lr'] * config['min_lr'],\n",
    "              'batch_size': 32,\n",
    "              'loss_1': 0.5,\n",
    "              'loss_2': 0.5\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CV\n",
    "# from sklearn.model_selection import GroupKFold,KFold\n",
    "# groups = (train_df['GameId'].astype(str).str[:4] + train_df['Week'].astype(str))\n",
    "# common_idx = np.where((train_df['Season'] == 2017).values[::22])[0]\n",
    "# groups = groups[train_df['Season'] != 2017].values[::22]\n",
    "# kf = GroupKFold(n_splits=3)\n",
    "# model_list = []\n",
    "# iso_reg = []\n",
    "# oof = np.zeros([len(train_X),199])\n",
    "# oof_iso = np.zeros([len(train_X),199])\n",
    "# batch_size = params['batch_size']\n",
    "# for fold_, (trn_idx_, val_idx_) in enumerate(kf.split(train_y[len(common_idx):], train_y[len(common_idx):],groups)):\n",
    "#     print(\"fold n{}\".format(fold_))\n",
    "#     trn_idx = np.append(common_idx,trn_idx_+len(common_idx))\n",
    "#     val_idx = val_idx_+len(common_idx)\n",
    "#     X_tr,y_tr,y_OH_tr = train_X[trn_idx], train_y[trn_idx],train_y_OH[trn_idx]\n",
    "#     X_val,y_val,y_OH_val = train_X[val_idx], train_y[val_idx],train_y_OH[val_idx]\n",
    "    \n",
    "#     tmp = deepcopy(y_OH_tr)\n",
    "#     y_OH_tr[:,:-1] += 0.5 * tmp[:,1:]\n",
    "#     y_OH_tr[:,1:] += 0.5 * tmp[:,:-1]\n",
    "    \n",
    "#     params['cosine_T'] = math.ceil(len(y_tr)/batch_size)\n",
    "#     clf,optimizer,scheduler = Create_model(params)\n",
    "#     best_loss = 9999\n",
    "#     Best_test_pred = []\n",
    "#     for epoch in tqdm_notebook(range(30)):\n",
    "#         clf.train()\n",
    "#         batches = np.arange(len(trn_idx))\n",
    "#         np.random.shuffle(batches)\n",
    "#         batches = batches[:batch_size * (len(batches)//batch_size)]\n",
    "#         batches = batches.reshape(-1,batch_size)\n",
    "#         Train_pred = []\n",
    "#         MA_loss = 0\n",
    "#         for batch in batches:\n",
    "#             optimizer.zero_grad()\n",
    "#             X = torch.Tensor(X_tr[batch]).to(device)\n",
    "#             y = torch.Tensor(y_tr[batch]).to(device)\n",
    "#             y_OH = torch.Tensor(y_OH_tr[batch]).to(device)\n",
    "#             y_pred_OH,y_pred = clf(X)\n",
    "#             loss = torch.mean((y - y_pred)**2)\n",
    "# #             loss2 = -torch.mean(y_OH * torch.log(1e-8 + y_pred_OH) + (1-y_OH) * torch.log(1-y_pred_OH+1e-8))\n",
    "#             loss2 = -torch.mean(y_OH * torch.log(1e-8 + y_pred_OH) + (1-y_OH) * torch.log(1-y_pred_OH+1e-8))\n",
    "# #             loss3 = torch.mean((y_OH - y_pred_OH)**2)\n",
    "# #             l2loss = ((clf.mlp[-1].weight[1:] - clf.mlp[-1].weight[:-1])**2).mean()\n",
    "# #             (params['loss_1']*loss+params['loss_2']*loss2 + l2loss).backward()\n",
    "#             (params['loss_1']*loss+params['loss_2']*loss2).backward()\n",
    "# #             mu,y_pred = clf(X)\n",
    "# #             loss = torch.mean((y - y_pred)**2)\n",
    "# #             loss2 = (torch.abs(mu - (y.argmax(-1).float()-99.5)/57.445626465380286)).mean()\n",
    "# #             (0.5*loss+0.5*loss2).backward()\n",
    "#             optimizer.step()\n",
    "#             Train_pred.append(y_pred.detach().cpu().reshape(-1))\n",
    "#             MA_loss += loss.item()\n",
    "#         if epoch >= 10:\n",
    "#             scheduler.step()\n",
    "#         Train_pred = torch.cat(Train_pred,dim=0).numpy()\n",
    "#         MA_loss /= len(batches)\n",
    "# #         plt.plot(y_pred_OH.detach().cpu().numpy()[0])\n",
    "# #         plt.show()\n",
    "#         with torch.no_grad():\n",
    "#             clf.eval()\n",
    "#             batches = np.arange(len(val_idx))\n",
    "#             last_batch = batches[batch_size * (len(batches)//batch_size):]\n",
    "#             batches = batches[:batch_size * (len(batches)//batch_size)]\n",
    "#             batches = batches.reshape(-1,batch_size)\n",
    "#             Test_loss = 0\n",
    "#             Test_pred = []\n",
    "#             for batch in batches:\n",
    "#                 X = torch.Tensor(X_val[batch]).to(device)\n",
    "#                 y = torch.Tensor(y_val[batch]).to(device)\n",
    "#                 y_pred = clf(X)[1]\n",
    "#                 loss = torch.mean((y - y_pred)**2)\n",
    "#                 Test_loss += loss.item() * batch_size\n",
    "#                 Test_pred.append(y_pred.detach().cpu().reshape(-1,199))\n",
    "#             if len(last_batch) > 0:\n",
    "#                 batch = last_batch\n",
    "#                 X = torch.Tensor(X_val[batch]).to(device)\n",
    "#                 y = torch.Tensor(y_val[batch]).to(device)\n",
    "#                 y_pred = clf(X)[1]\n",
    "#                 loss = torch.mean((y - y_pred)**2)\n",
    "#                 Test_loss += loss.item() * batch_size\n",
    "#                 Test_pred.append(y_pred.detach().cpu().reshape(-1,199))\n",
    "#             Test_loss /= len(val_idx)\n",
    "#             Test_pred = torch.cat(Test_pred,dim=0).numpy()\n",
    "#             if Test_loss < best_loss:\n",
    "#                 Best_weight = clf.state_dict()\n",
    "#                 Best_weight = {name : Best_weight[name].cpu() for name in Best_weight}\n",
    "#                 best_loss = Test_loss\n",
    "#             print(\"epoch\",epoch,\"Train:\",MA_loss,\"Test:\",Test_loss,\"Best:\",best_loss)\n",
    "#         if epoch >= 10 and epoch%2 == 1:\n",
    "#             Best_test_pred.append(np.array(Test_pred.copy(), dtype=np.float64))\n",
    "#             model_list.append(deepcopy(clf))\n",
    "        \n",
    "#     Best_test_pred = np.mean(Best_test_pred,0)\n",
    "#     oof[val_idx] = Best_test_pred\n",
    "#     print(((1 - Yard_mask[val_idx])*(oof[val_idx] - train_y[val_idx])**2).mean())\n",
    "\n",
    "# print(((1 - Yard_mask[11900:])*(oof[11900:] - train_y[11900:])**2).mean())\n",
    "# print(((1 - Yard_mask[-3438:])*(oof[-3438:] - train_y[-3438:])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_Variables: 1098567\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55973fedf1141d0b49dd7a4f820187a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 Train: 0.016250557515729035\n",
      "epoch 1 Train: 0.013002622230055624\n",
      "epoch 2 Train: 0.012853930019953632\n",
      "epoch 3 Train: 0.012681736015246471\n",
      "epoch 4 Train: 0.012624602738474574\n",
      "epoch 5 Train: 0.012520734660034146\n",
      "epoch 6 Train: 0.012436979121621184\n",
      "epoch 7 Train: 0.012421118066574123\n",
      "epoch 8 Train: 0.012356209761632904\n",
      "epoch 9 Train: 0.01229370125885847\n",
      "epoch 10 Train: 0.012211518966752356\n",
      "epoch 11 Train: 0.012015246002142477\n",
      "epoch 12 Train: 0.01213232027199531\n",
      "epoch 13 Train: 0.011954837246509835\n",
      "epoch 14 Train: 0.012096115759090207\n",
      "epoch 15 Train: 0.01188300004054005\n",
      "epoch 16 Train: 0.01203598197194547\n",
      "epoch 17 Train: 0.011842479867582821\n",
      "epoch 18 Train: 0.012022248765155261\n",
      "epoch 19 Train: 0.011746972078941145\n",
      "epoch 20 Train: 0.011958495079596315\n",
      "epoch 21 Train: 0.011709122422655519\n",
      "epoch 22 Train: 0.011908630268051397\n",
      "epoch 23 Train: 0.011660106068487325\n",
      "epoch 24 Train: 0.011862349192205385\n",
      "epoch 25 Train: 0.011580199563428142\n",
      "epoch 26 Train: 0.011832864719664276\n",
      "epoch 27 Train: 0.011623000954432177\n",
      "epoch 28 Train: 0.011772205450479455\n",
      "epoch 29 Train: 0.011550992343945233\n",
      "\n",
      "Num_Variables: 1098567\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23a62dd640140c3bef8fb7fac305242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 Train: 0.01618642760929523\n",
      "epoch 1 Train: 0.013034478323894311\n",
      "epoch 2 Train: 0.012811590422804144\n",
      "epoch 3 Train: 0.012682979766958655\n",
      "epoch 4 Train: 0.012602103955127834\n",
      "epoch 5 Train: 0.01252070854055227\n",
      "epoch 6 Train: 0.012471022923703393\n",
      "epoch 7 Train: 0.012444139569508971\n",
      "epoch 8 Train: 0.012344851316781251\n",
      "epoch 9 Train: 0.012293871845252482\n",
      "epoch 10 Train: 0.012273997669777117\n",
      "epoch 11 Train: 0.012008431556027445\n",
      "epoch 12 Train: 0.012202163953051095\n",
      "epoch 13 Train: 0.011963650407346779\n",
      "epoch 14 Train: 0.012106919840611106\n",
      "epoch 15 Train: 0.011884814505825509\n",
      "epoch 16 Train: 0.012062151657767306\n",
      "epoch 17 Train: 0.011838405256710165\n",
      "epoch 18 Train: 0.011999460241438317\n",
      "epoch 19 Train: 0.011810143858976456\n",
      "epoch 20 Train: 0.011969439388839374\n",
      "epoch 21 Train: 0.011726456332838585\n",
      "epoch 22 Train: 0.011908815189609458\n",
      "epoch 23 Train: 0.011644204309884233\n",
      "epoch 24 Train: 0.01189302258741481\n",
      "epoch 25 Train: 0.01167519184701577\n",
      "epoch 26 Train: 0.011858866449049288\n",
      "epoch 27 Train: 0.01162194855156154\n",
      "epoch 28 Train: 0.01178289073498827\n",
      "epoch 29 Train: 0.011584272864146044\n",
      "\n",
      "Num_Variables: 1098567\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bfee8d442640aba9242ee33d792b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 Train: 0.016135400747725976\n",
      "epoch 1 Train: 0.013027783395794544\n",
      "epoch 2 Train: 0.012842109908596792\n",
      "epoch 3 Train: 0.012699492830134804\n",
      "epoch 4 Train: 0.01261301872575172\n",
      "epoch 5 Train: 0.012516204125597785\n",
      "epoch 6 Train: 0.012437365163832861\n",
      "epoch 7 Train: 0.01240759673566964\n",
      "epoch 8 Train: 0.012308934055145274\n",
      "epoch 9 Train: 0.012290495526925429\n",
      "epoch 10 Train: 0.012240547558179547\n",
      "epoch 11 Train: 0.012043592278223011\n",
      "epoch 12 Train: 0.012173047585114425\n",
      "epoch 13 Train: 0.01196403183925444\n",
      "epoch 14 Train: 0.012118097928543035\n",
      "epoch 15 Train: 0.011927695988641424\n",
      "epoch 16 Train: 0.012056570499492244\n",
      "epoch 17 Train: 0.01178159981062996\n",
      "epoch 18 Train: 0.012048429518150813\n",
      "epoch 19 Train: 0.011773253222464362\n",
      "epoch 20 Train: 0.011981093394827324\n",
      "epoch 21 Train: 0.011700848439703206\n",
      "epoch 22 Train: 0.0119448184660888\n",
      "epoch 23 Train: 0.011680012481359157\n",
      "Overtime\n"
     ]
    }
   ],
   "source": [
    "model_list = []\n",
    "oof = np.zeros([len(train_X),199])\n",
    "batch_size = params['batch_size']\n",
    "for fold_ in range(999):\n",
    "    trn_idx = np.arange(len(train_X))\n",
    "    X_tr,y_tr,y_OH_tr = train_X[trn_idx], train_y[trn_idx],train_y_OH[trn_idx]\n",
    "    tmp = deepcopy(y_OH_tr)\n",
    "    y_OH_tr[:,:-1] += 0.5 * tmp[:,1:]\n",
    "    y_OH_tr[:,1:] += 0.5 * tmp[:,:-1]\n",
    "    params['cosine_T'] = len(y_tr)//batch_size\n",
    "    clf,optimizer,scheduler = Create_model(params)\n",
    "    best_loss = 9999\n",
    "    for epoch in tqdm_notebook(range(30)):\n",
    "        clf.train()\n",
    "        batches = np.arange(len(trn_idx))\n",
    "        np.random.shuffle(batches)\n",
    "        batches = batches[:batch_size * (len(batches)//batch_size)]\n",
    "        batches = batches.reshape(-1,batch_size)\n",
    "        Train_pred = []\n",
    "        MA_loss = 0\n",
    "        for batch in batches:\n",
    "            optimizer.zero_grad()\n",
    "            X = torch.Tensor(X_tr[batch]).to(device)\n",
    "            y = torch.Tensor(y_tr[batch]).to(device)\n",
    "            y_OH = torch.Tensor(y_OH_tr[batch]).to(device)\n",
    "            y_pred_OH,y_pred = clf(X)\n",
    "            loss = torch.mean((y - y_pred)**2)\n",
    "            loss2 = -torch.mean(y_OH * torch.log(1e-8 + y_pred_OH) + (1-y_OH) * torch.log(1-y_pred_OH+1e-8))\n",
    "#             l2loss = ((clf.mlp[-1].weight[1:] - clf.mlp[-1].weight[:-1])**2).mean()\n",
    "#             (params['loss_1']*loss+params['loss_2']*loss2 + 0.1*l2loss).backward()\n",
    "            (params['loss_1']*loss+params['loss_2']*loss2).backward()\n",
    "            optimizer.step()\n",
    "            MA_loss += loss.item()\n",
    "        if epoch >= 10:\n",
    "            scheduler.step()\n",
    "        MA_loss /= len(batches)\n",
    "        print(\"epoch\",epoch,\"Train:\",MA_loss)\n",
    "        if epoch >= 10 and epoch%2 == 1:\n",
    "            model_list.append(deepcopy(clf))\n",
    "        if time.time()-start_time > time_limit:\n",
    "            break\n",
    "    if time.time()-start_time > time_limit:\n",
    "        print(\"Overtime\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7f38bd97374d30a12cd30755861676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your submission file has been saved!  Once you `Commit` your Notebook and it finishes running, you can submit the file to the competition from the Notebook Viewer `Output` tab.\n"
     ]
    }
   ],
   "source": [
    "for (test_df, sample_prediction_df) in tqdm_notebook(env.iter_test()):\n",
    "    test_X = Create_group_features(test_df.copy(),train=False)\n",
    "    yardline = test_X['YardLine'].iloc[-1]\n",
    "    test_X[bool_cols] = test_X[bool_cols].astype(np.float32)\n",
    "    test_X[Scale_cols] = (test_X[Scale_cols] - X_mean[Scale_cols])/X_std[Scale_cols]\n",
    "    test_X = torch.Tensor(test_X.fillna(0).values.reshape(-1,22,train_X.shape[-1]))\n",
    "    y_pred_p = np.zeros(199)\n",
    "    for i,model in enumerate(model_list):\n",
    "        model.eval()\n",
    "        pred = model(test_X)[1][-1].detach().numpy()\n",
    "        y_pred_p += pred / len(model_list)\n",
    "    y_pred_p = np.clip(y_pred_p,0,1)\n",
    "    y_pred_p[:int(99-yardline)] = 0\n",
    "    y_pred_p[int(199-yardline):] = 1\n",
    "    env.predict(pd.DataFrame(data=[y_pred_p],columns=sample_prediction_df.columns))\n",
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06332cd2e6cb4385bb6f6df6bcd109bf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "11986aa30792487e8792c7bb7c1ab558": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1202569f5ab84059ba7477b92ef1a0af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18ebe6fffee84c268113506bf5a3cc1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_64b067fbaad043ee8db52ac0bc259306",
       "placeholder": "",
       "style": "IPY_MODEL_37b5b29ff8b04fdcafe16e1148ecf2bf",
       "value": " 30/30 [53:31&lt;00:00, 107.06s/it]"
      }
     },
     "1904b8b7b4264a96aacc77a1f575118d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4f7ec4da4c7f4b50ab3cf9c5c4eb9f68",
       "max": 30,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_241a39b044b64422aaf57acda4d7ae7b",
       "value": 30
      }
     },
     "241a39b044b64422aaf57acda4d7ae7b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "2765024e971a41b1ae6533677c537f56": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "27bd587c0179454a85b40b69a9d6701a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_642fa0d302464a7b963fba1865de4f69",
       "placeholder": "",
       "style": "IPY_MODEL_ff7f3062ca3441ec91af4bc8c4ac1ff8",
       "value": " 23/30 [42:36&lt;12:24, 106.38s/it]"
      }
     },
     "2913a57b340f4f32b32a3bd9a1e803b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "2b4a6032a51846b1b7e78802b1214bdb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "36a2a05c69c24116a6cb1e1f90803d10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "37b5b29ff8b04fdcafe16e1148ecf2bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "41e8c09f8dec40b784fae3490a5d01b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1202569f5ab84059ba7477b92ef1a0af",
       "max": 30,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2765024e971a41b1ae6533677c537f56",
       "value": 30
      }
     },
     "4f7ec4da4c7f4b50ab3cf9c5c4eb9f68": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "642fa0d302464a7b963fba1865de4f69": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "64b067fbaad043ee8db52ac0bc259306": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "97cbeacead704d42a79c02b7825950c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a23a62dd640140c3bef8fb7fac305242": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1904b8b7b4264a96aacc77a1f575118d",
        "IPY_MODEL_18ebe6fffee84c268113506bf5a3cc1d"
       ],
       "layout": "IPY_MODEL_2b4a6032a51846b1b7e78802b1214bdb"
      }
     },
     "ac2d32514cc64821817f4afd2d2199ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f1d4bf0bc18941d2a8432feb0022a35b",
       "placeholder": "",
       "style": "IPY_MODEL_be8ea9f8cb054ee2a164dde48bc6b42b",
       "value": " 3438/? [29:41&lt;00:00,  1.93it/s]"
      }
     },
     "ad71dbc65ea54bcfa539c1fd076a894b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9000fe6f01942aa86d51e8e4ce0c8b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bc7f38bd97374d30a12cd30755861676": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f562f833049d4c1c867d44e099a470ef",
        "IPY_MODEL_ac2d32514cc64821817f4afd2d2199ee"
       ],
       "layout": "IPY_MODEL_11986aa30792487e8792c7bb7c1ab558"
      }
     },
     "be8ea9f8cb054ee2a164dde48bc6b42b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ceeb3ae4f3b24be0ae6d1b3e3be06551": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d3bfee8d442640aba9242ee33d792b81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d71de7fd074b43efb0c89a31d64aa0e7",
        "IPY_MODEL_27bd587c0179454a85b40b69a9d6701a"
       ],
       "layout": "IPY_MODEL_b9000fe6f01942aa86d51e8e4ce0c8b5"
      }
     },
     "d55973fedf1141d0b49dd7a4f820187a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_41e8c09f8dec40b784fae3490a5d01b1",
        "IPY_MODEL_e69e385795104e2c95982854cba2cecc"
       ],
       "layout": "IPY_MODEL_06332cd2e6cb4385bb6f6df6bcd109bf"
      }
     },
     "d71de7fd074b43efb0c89a31d64aa0e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": " 77%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ceeb3ae4f3b24be0ae6d1b3e3be06551",
       "max": 30,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2913a57b340f4f32b32a3bd9a1e803b1",
       "value": 23
      }
     },
     "e69e385795104e2c95982854cba2cecc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ad71dbc65ea54bcfa539c1fd076a894b",
       "placeholder": "",
       "style": "IPY_MODEL_eac17456b6b541e9abbc6ab500a714f5",
       "value": " 30/30 [2:59:24&lt;00:00, 358.82s/it]"
      }
     },
     "eac17456b6b541e9abbc6ab500a714f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f1d4bf0bc18941d2a8432feb0022a35b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f562f833049d4c1c867d44e099a470ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_97cbeacead704d42a79c02b7825950c4",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_36a2a05c69c24116a6cb1e1f90803d10",
       "value": 1
      }
     },
     "ff7f3062ca3441ec91af4bc8c4ac1ff8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
